---
title: "Application"
description: |
  Une application fil rouge pour illustrer l'intérêt d'appliquer graduellement les bonnes pratiques dans une optique de mise en production d'une application de data science.
order: 10
href: chapters/application.html
image: /rocket.png
---

<details>
<summary>
Dérouler les _slides_ ci-dessous ou [cliquer ici](https://ensae-reproductibilite.github.io/slides/#/title-slide)
pour afficher les slides en plein écran.
</summary>


<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://ensae-reproductibilite.github.io/slides/#/title-slide"></iframe></div>

</details>

L'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.

Celle-ci est un tutoriel pas à pas pour avoir un projet reproductible et disponible sous plusieurs livrables.
Toutes les étapes ne sont pas indispensables à tous les projets de _data science_.

Nous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science.
On a un _notebook_ un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :

- Import de données ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entraînement d'un modèle ;
- Evaluation du modèle.

**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée.**


<details>
<summary>
Illustration de notre point de départ
</summary>
![](/drawio/starting_point.png)
</details>

<details>
<summary>
Illustration de l'horizon vers lequel on se dirige
</summary>
![](/drawio/end_point.png)
</details>

::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines étapes peuvent être rapides, d'autres plus fastidieuses ;
certaines être assez guidées, d'autres vous laisser plus de liberté.
Si vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à
l'étape suivante qui en dépend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous
recommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les
outils nécessaires sont pré-installés et pré-configurés. Le service `VSCode`
ne sera en effet que le point d'entrée pour l'utilisation d'outils plus exigeants
sur le plan de l'infrastructure: _Argo_, _MLFLow_, etc.
:::


# Partie 0 : initialisation du projet

{{< include "./applications/_appli0.qmd" >}}

# Partie 1 : qualité du script

Cette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours.

Elle fait intervenir les notions suivantes :

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.qmd)) ;
- **Qualité du code** (voir [Qualité du code](/chapters/code-quality.qmd)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contrôle de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).


Le plan de la partie est le suivant :

1. S'assurer que le script fonctionne ;
2. Nettoyer le code des scories formelles avec un _linter_ et un _formatter_ ;
3. Paramétrisation du script ;
4. Utilisation de fonctions.


## Étape 1 : s'assurer que le script s'exécute correctement

On va partir du fichier `notebook.py` qui reprend le contenu
du _notebook_[^jupytext] mais dans un script classique.
Le travail de nettoyage en sera facilité.

[^jupytext]: L'export dans un script `.py` a été fait
        directement depuis `VSCode`. Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette étape et fournissons
        directement le script expurgé du texte intermédiaire. Mais n'oubliez
        pas que cette démarche, fréquente quand on a démarré sur un _notebook_ et
        qu'on désire consolider en faisant la transition vers des
        scripts, nécessite d'être attentif pour ne pas risquer de faire une erreur.

La première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**.
Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans `VSCode`
et un terminal pour le lancer.


{{< include "./applications/_appli1.qmd" >}}


## Étape 2: utiliser un _linter_ puis un _formatter_

On va maintenant améliorer la qualité de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et le _formatter_ [`Black`](https://github.com/psf/black).
Si vous désirez un outil deux en un, il est possible d'utiliser [`Ruff`](https://github.com/astral-sh/ruff-vscode)
en complément ou substitut.

Ce nettoyage automatique du code permettra, au passage, de restructurer notre
script de manière plus naturelle.

::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)
sont des _packages_ `Python` qui
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui suggère
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)
ou [`Black`](https://black.readthedocs.io/en/stable/),
n'oubliez pas d'exécuter la commande `pip install pylint` ou `pip install black`.
:::


Le _linter_ renvoie alors une série d'irrégularités,
en précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualité du code à l'aune des standards communautaires évoqués
dans la partie [Qualité du code](/chapters/code-quality.html).

{{< include "./applications/_appli2.qmd" >}}

Le code est maintenant lisible, il obtient à ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment
quelques redondances de code auxquelles nous allons nous attaquer par la suite.
Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script:
jetons d'API et chemin des fichiers.


## Étape 3: gestion des paramètres

L'exécution du code et les résultats obtenus
dépendent de certains paramètres définis dans le code. L'étude de résultats
alternatifs, en jouant sur
des variantes des (hyper)paramètres, est à ce stade compliquée
car il est nécessaire de parcourir le code pour trouver
ces paramètres. De plus, certains paramètres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation à
être présents dans le code.

Il est plus judicieux de considérer ces paramètres comme des
variables d'entrée du script. Cela peut être fait de deux
manières:

1. Avec des __arguments optionnels__ appelés depuis la ligne de commande _(Application 3a)_.
Cela peut être pratique pour mettre en oeuvre des tests automatisés mais
n'est pas forcément pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un __fichier de configuration__ dont les valeurs sont importées dans
le script principal _(Application 3b)_.


<details>
<summary>
Un exemple de définition d'un argument pour l'utilisation en ligne de commande
</summary>

```{.python filename="prenom.py"}
import argparse
parser = argparse.ArgumentParser(description="Qui êtes-vous?")
parser.add_argument(
    "--prenom", type=str, default="Toto", help="Un prénom à afficher"
)
args = parser.parse_args()
print(args.prenom)
```

Exemples d'utilisations en ligne de commande

```{.bash filename="terminal"}
python prenom.py
python prenom.py --prenom "Zinedine"
```

</details>

{{< include "./applications/_appli3.qmd" >}}


## Étape 4 : Privilégier la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse**.
Ceci facilitera l'étape ultérieure de modularisation de notre projet.

Cet exercice étant chronophage, il n'est __pas obligatoire de le réaliser en entier__. L'important est de
comprendre la démarche et d'adopter fréquemment une approche fonctionnelle[^POO]. Pour obtenir
une chaine entièrement fonctionnalisée, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orientée objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au développeur. L'arbitrage coût-avantage est négatif pour notre
exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d'un modèle,
il est recommandé de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/).

{{< include "./applications/_appli4.qmd" >}}

Cela ne se remarque pas encore vraiment car nous avons de nombreuses définitions de fonctions
mais notre chaine de production est beaucoup plus
concise (le script fait environ 300 lignes dont 250 de définitions de fonctions génériques).
Cette auto-discipline facilitera grandement
les étapes ultérieures. Cela aurait été néanmoins beaucoup moins coûteux en temps d'adopter
ces bons gestes de manière plus précoce.


# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est déjà considérablement rapprochés d'un
possible partage du code : celui-ci est lisible et intelligible.
Le code est proprement versionné sur un
dépôt `GitHub`.
Cependant, le projet est encore perfectible: il est encore difficile de rentrer
dedans si on ne sait pas exactement ce qu'on recherche. L'objectif de cette partie
est d'isoler les différentes étapes de notre _pipeline_.
Outre le gain de clarté pour notre projet, nous économiserons beaucoup de peines
pour la mise en production ultérieure de notre modèle.

<details>
<summary>
Illustration de l'état actuel du projet
</summary>
![](/schema_post_appli4.png)
</details>

Dans cette partie nous allons continuer les améliorations
incrémentales de notre projet avec les étapes suivantes:

1. Modularisation du code `Python` pour séparer les différentes
étapes de notre _pipeline_ ;
2. Adopter une structure standardisée pour notre projet afin
d'autodocumenter l'organisation de celui-ci ;
3. Documenter les _packages_ indispensables à l'exécution du code ;
4. Stocker les données dans un environnement adéquat
afin de continuer la démarche de séparer conceptuellement les données du code en de la configuration.


## Étape 1 : modularisation

Nous allons profiter de la modularisation pour adopter une structure
applicative pour notre code. Celui-ci n'étant en effet plus lancé
que depuis la ligne de commande, on peut considérer qu'on construit
une application générique où un script principal (`main.py`)
encapsule des éléments issus d'autres scripts `Python`.


{{< include "./applications/_appli5.qmd" >}}


## Étape 2 : adopter une architecture standardisée de projet

On dispose maintenant d'une application `Python` fonctionnelle.
Néanmoins, le projet est certes plus fiable mais sa structuration
laisse à désirer et il serait difficile de rentrer à nouveau
dans le projet dans quelques temps.

<details>
<summary>Etat actuel du projet 🙈</summary>

```
├── .gitignore
├── data.csv
├── train.csv
├── test.csv
├── README.md
├── config.yaml
├── import_data.py
├── build_features.py
├── train_evaluate.py
├── titanic.ipynb
└── main.py
```


</details>

Comme cela est expliqué dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va
faciliter l'autodocumentation de notre projet. De plus, une telle structure va faciliter des évolutions optionnelles
comme la _packagisation_ du projet. Passer d'une structure modulaire
bien faite à un _package_ est quasi-immédiat en `Python`.

On va donc modifier l'architecture de notre projet pour la rendre plus standardisée.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui génèrent des _templates_ de projet. En l'occurrence
notre source d'inspiration sera le [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
issu d'un effort communautaire.

::: {.callout-note}
L'idée de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante :

```{.bash filename="terminal"}
pip install cookiecutter
cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

<details>
<summary>
Structure recommandée
</summary>

```
application
├── main.py
├── README.md
├── data
│   ├── raw
│   │   └── data.csv
│   └── derived
│       ├── test.csv
│       └── train.csv
├── configuration
│   └── config.yaml
├── notebooks
│   └── titanic.ipynb
└── src
    ├── data
    │   └── import_data.py
    ├── pipeline
    │   └── build_pipeline.py
    └── models
        └── train_evaluate.py
```

</details>

{{< include "./applications/_appli6.qmd" >}}


## Étape 3: mieux tracer notre chaine de production

### Indiquer l'environnement minimal de reproductibilité

Le script `main.py` nécessite un certain nombre de packages pour
être fonctionnel. Chez vous les packages nécessaires sont
bien sûr installés mais êtes-vous assuré que c'est le cas
chez la personne qui testera votre code ?

Afin de favoriser la portabilité du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-à-dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version.
Nous proposons de créer un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacrée aux environnements reproductibles.

Le fichier `requirements.txt` est conventionnellement localisé à la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier ultérieurement.

{{< include "./applications/_appli7a.qmd" >}}

### Tracer notre chaîne

Quand votre projet passera en production, vous aurez un accès limité à celui-ci. Il est donc important de faire remonter, par le biais du _logging_ des informations critiques sur votre projet qui vous permettront de savoir où il en est (si vous avez accès à la console où il tourne) ou là où il s'est arrêté.

L'utilisation de `print` montre rapidement ses limites pour cela. Les informations enregistrées ne persistent pas après la session et sont quelques peu rudimentaires.

Pour faire du _logging_, la librairie consacrée depuis longtemps en `Python` est... [`logging`](https://docs.python.org/3/library/logging.html). On va néanmoins ici proposer d'utiliser [`loguru`](https://github.com/Delgan/loguru) qui est un peu plus simple à configurer (l'instanciation du _logger_ est plus aisée) et plus agréable grâce à ses messages en couleurs qui permettent de visuellement trier les informations.

![](https://raw.githubusercontent.com/Delgan/loguru/master/docs/_static/img/demo.gif)

{{< include "./applications/_appli7b.qmd" >}}


## Étape 4 : stocker les données de manière externe {#stockageS3}

L'étape précédente nous a permis d'isoler la configuration. Nous avons conceptuellement isolé les données du code lors des applications précédentes. Cependant, nous n'avons pas été au bout du chemin car le stockage des données reste conjoint à celui du code. Nous allons maintenant dissocier ces deux éléments.

::: {.callout-warning collapse="true"}
## Pour en savoir plus sur le système de stockage `S3`

Pour mettre en oeuvre cette étape, il peut être utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://inseefrlab.github.io/docs.sspcloud.fr/docs/fr/storage.html) pour la réaliser. Une aide-mémoire est également disponible dans le cours
de 2e année de l'ENSAE [Python pour la _data science_](https://linogaliana-teaching.netlify.app/reads3/#).
:::


Le chapitre sur la [structure des projets](/chapters/projects-architecture.qmd)
développe l'idée qu'il est recommandé de converger vers un modèle
où environnements d'exécution, de stockage du code et des données sont conceptuellement
séparés. Ce haut niveau d'exigence est un gain de temps important
lors de la mise en production car au cours de cette dernière, le projet
est amené à être exécuté sur une infrastructure informatique dédiée
qu'il est bon d'anticiper. Schématiquement, nous visons la structure de projet suivante:

![](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/img/environment_clean.png)

A l'heure actuelle, les données sont stockées dans le dépôt. C'est une
mauvaise pratique. En premier lieu, `Git` n'est techniquement
pas bien adapté au stockage de données. Ici ce n'est pas très grave
car il ne s'agit pas de données volumineuses et ces dernières ne sont
pas modifiées au cours de notre chaine de traitement.

La raison principale
est que les données traitées par les _data scientists_
sont généralement soumises à des clauses de
confidentialités ([RGPD](https://www.cnil.fr/fr/rgpd-de-quoi-parle-t-on), [secret statistique](https://www.insee.fr/fr/information/1300624)...). Mettre ces données sous contrôle de version
c'est prendre le risque de les divulguer à un public non habilité.
Il est donc recommandé de privilégier des outils techniques adaptés au
stockage de données.

L'idéal, dans notre cas, est d'utiliser une solution de stockage externe.
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud.
Cela nous permettra de supprimer les données de `Github` tout en maintenant la reproductibilité
de notre projet [^history].

[^history]: Attention, les données ont été _committées_ au moins une fois. Les supprimer
du dépôt ne les efface pas de l'historique. Si cette erreur arrive, le mieux est de supprimer
le dépôt en ligne, créer un nouvel historique `Git` et partir de celui-ci pour des publications
ultérieures sur `Github`. Néanmoins l'idéal serait de ne pas s'exposer à cela. C'est justement
l'objet des bonnes pratiques de ce cours: un `.gitignore` bien construit et une séparation des
environnements de stockage du code et
des données seront bien plus efficaces pour vous éviter ces problèmes que tout les conseils de
vigilance que vous pourrez trouver ailleurs.

Plus concrètement, nous allons adopter le pipeline suivant pour notre projet:

![](/chapters/applications/figures/pipeline_appli8.png)

Le scénario type est que nous avons une source brute, reçue sous forme de CSV, dont on ne peut changer le format. Il aurait été idéal d'avoir un format plus adapté au traitement de données pour ce fichier mais ce n'était pas de notre ressort. Notre chaine va aller chercher ce fichier, travailler dessus jusqu'à valoriser celui-ci sous la forme de notre matrice de confusion. Si on imagine que notre chaine prend un certain temps, il n'est pas inutile d'écrire des données intermédiaires. Pour faire cela, puisque nous avons la main, autant choisir un format adapté, à savoir le format `Parquet`.


{{< include "./applications/_appli8.qmd" >}}


# Partie 2bis: packagisation de son projet (optionnel)

Cette série d'actions n'est pas forcément pertinente pour tous
les projets. Elle fait un peu la transition entre la modularité
et la portabilité.

## Étape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions génériques.
On peut vouloir tester leur usage sur des données standardisées,
différentes de celles du Titanic.

Même si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut être pédagogique.

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
pour effectuer des tests unitaires. Cette approche nécessite quelques notions
de programmation orientée objet ou une bonne discussion avec `ChatGPT`.

{{< include "./applications/_appli9.qmd" >}}


::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche généralement
à tester le plus de lignes possibles de son code. On parle de
__taux de couverture__ (_coverage rate_) pour désigner
la statistique mesurant cela.

Cela peut s'effectuer de la manière suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```{.bash filename="terminal"}
coverage run -m unittest tests/test_create_variable_title.py
coverage report -m
```

```{.python}
Name                                  Stmts   Miss  Cover   Missing
-------------------------------------------------------------------
src/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113
tests/test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------------
TOTAL                                    55     22    60%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualité. Il existe d'ailleurs
des badges `Github` dédiés.
:::




## Étape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple à transformer
en _package_, en s'inspirant de la structure du `cookiecutter` adapté, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

On va créer un _package_ nommé `titanicml` qui encapsule
tout notre code et qui sera appelé
par notre script `main.py`. La structure attendue
est la suivante:

<details>
<summary>Structure visée</summary>

```
ensae-reproductibilite-application
├── docs                                    ┐
│   ├── main.py                             │
│   └── notebooks                           │ Package documentation and examples
│       └── titanic.ipynb                   │
├── configuration                           ┐ Configuration (pas à partager avec Git)
│   └── config.yaml                         ┘
├── README.md
├── pyproject.toml                          ┐
├── requirements.txt                        │
├── titanicml                               │
│   ├── __init__.py                         │ Package source code, metadata
│   ├── data                                │ and build instructions
│   │   ├── import_data.py                  │
│   │   └── test_create_variable_title.py   │
│   ├── features                            │
│   │   └── build_features.py               │
│   └── models                              │
│       └── train_evaluate.py               ┘
└── tests                                   ┐
    └── test_create_variable_title.py       ┘ Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```
ensae-reproductibilite-application
├── notebooks
│   └── titanic.ipynb
├── configuration
│   └── config.yaml
├── main.py
├── README.md
├── requirements.txt
└── src
    ├── data
    │   ├── import_data.py
    │   └── test_create_variable_title.py
    ├── features
    │   └── build_features.py
    └── models
        └── train_evaluate.py
```
</details>

Il existe plusieurs
_frameworks_ pour
construire un _package_. Nous
allons privilégier [`Poetry`](https://python-poetry.org/)
à [`Setuptools`](https://pypi.org/project/setuptools/).


::: {.callout-note}

Pour créer la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapté,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a déjà une structure très modulaire, on va plutôt recréer cette
structure dans notre projet déjà existant. En fait, il ne manque qu'un fichier essentiel,
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```{.bash filename="terminal"}
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>Dérouler pour voir les choix possibles</summary>
```{.python}
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]:
python_version [3.9]:
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]:
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::

{{< include "./applications/_appli10.qmd" >}}

# Partie 3 : construction d'un projet portable et reproductible {#partie3}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues
dans les chapitres [Qualité du code](/chapters/code-quality.html)
et [Structure des projets](/chapters/projects-architecture.html)
tout au long du cours.

Ce faisant, on s'est déjà considérablement rapprochés d'une
possible mise en production : le code est lisible,
la structure du projet est normalisée et évolutive,
et le code est proprement versionné sur un
dépôt `GitHub` {{< fa brands github >}}.


<details>
<summary>
Illustration de l'état actuel du projet
</summary>
![](/chapters/applications/figures/_pipeline_avant_partie3.png)

</details>



A présent, nous avons une version du projet qui est largement partageable.
Du moins en théorie, car la pratique est souvent plus compliquée :
il y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.

Dans cette troisème partie de notre travail vers la mise en production,
nous allons voir
comment **normaliser l'environnement d'exécution afin de produire un projet portable**.
Autrement dit, nous n'allons plus nous contenter de modularité mais allons rechercher
la portabilité.
On sera alors tout proche de pouvoir mettre le projet en production.

On progressera dans l'échelle de la reproductibilité
de la manière suivante:

1. [**Environnements virtuels**](#anaconda) ;
2. Créer un [script shell](#shell) qui permet, depuis un environnement minimal, de construire l'application de A à Z ;
3. [**Images et conteneurs `Docker`**](#docker).


Nous allons repartir de l'application 8, c'est-à-dire d'un projet
modulaire mais qui n'est pas, à strictement parler, un _package_
(objet des applications optionnelles suivantes 9 et 10).

Pour se replacer dans l'état du projet à ce niveau,
il est possible d'utiliser le _tag_ _ad hoc_.

```{.bash filename="terminal"}
git stash
git checkout appli8
```


## Étape 1 : un environnement pour rendre le projet portable {#anaconda}

Pour qu'un projet soit portable, il doit remplir deux conditions:

- Ne pas nécessiter de dépendance
qui ne soient pas renseignées quelque part ;
- Ne pas proposer des dépendances inutiles, qui ne
sont pas utilisées dans le cadre du projet.

Le prochain exercice vise à mettre ceci en oeuvre.
Comme expliqué dans le [chapitre portabilité](/chapters/portability.qmd),
le choix du gestionnaire d'environnement est laissé
libre. Il est recommandé de privilégier `venv` si vous découvrez
la problématique de la portabilité.

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

L'approche la plus légère est l'environnement virtuel.
Nous avons en fait implicitement déjà commencé à aller vers
cette direction
en créant un fichier `requirements.txt`.

{{< include "./applications/_appli11a.qmd" >}}


## Environnement `conda`

Les environnements `conda` sont plus lourds à mettre en oeuvre que les
environnements virtuels mais peuvent permettre un contrôle
plus formel des dépendances.

{{< include "./applications/_appli11b.qmd" >}}

:::


## Étape 2: construire l'environnement de notre application via un script `shell` {#shell}

Les environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L'idée est de construire une machine, en partant d'une base quasi-vierge, qui permette de construire étape par étape l'environnement nécessaire au bon fonctionnement de notre projet. C'est le principe des conteneurs `Docker` {{< fa brands docker >}}.

Leur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production.

- Nous allons d'abord créer un script `shell`, c'est à dire une suite de commandes `Linux` permettant de construire l'environnement à partir d'une machine vierge ;
- Nous transformerons celui-ci en `Dockerfile` dans un deuxième temps. C'est l'objet de l'étape suivante.

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

{{< include "./applications/_appli12a.qmd" >}}

## Environnement `conda`

{{< include "./applications/_appli12b.qmd" >}}

:::


## Étape 3: conteneuriser l'application avec `Docker` {#docker}


::: {.callout-note}
Cette application nécessite l'accès à une version interactive de `Docker`.
Il n'y a pas beaucoup d'instances en ligne disponibles.

Nous proposons deux solutions:

- [Installer `Docker`](https://docs.docker.com/get-docker/) sur sa machine ;
- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_

Sinon, elle peut être réalisée en essai-erreur par le biais des services d'intégration continue de `Github` {{< fa brands github >}} ou `Gitlab` {{< fa brands gitlab >}}. Néanmoins, nous présenterons l'utilisation de ces services plus tard, dans la prochaine partie.
:::

Maintenant qu'on sait que ce script préparatoire fonctionne, on va le transformer en `Dockerfile` pour anticiper la mise en production.  Comme la syntaxe `Docker` est légèrement différente de la syntaxe `Linux` classique (voir le [chapitre portabilité](/chapters/portability.qmd)), il va être nécessaire de changer quelques instructions mais ceci sera très léger.

On va tester le `Dockerfile` dans un environnement bac à sable pour ensuite
pouvoir plus facilement automatiser la construction de l'image
`Docker`.

{{< include "./applications/_appli13.qmd" >}}


# Partie 4 : automatisation avec l'intégration continue


Imaginez que vous êtes au restaurant
et qu'on ne vous serve pas le plat mais seulement la recette
et que, de plus, on vous demande de préparer le plat
chez vous avec les ingrédients dans votre frigo.
Vous seriez quelque peu déçu. En revanche, si vous avez goûté
au plat, que vous êtes un réel cordon bleu
et qu'on vous donne la recette pour refaire ce plat ultérieurement,
peut-être
que vous appréciriez plus.

Cette analogie illustre l'enjeu de définir
le public cible et ses attentes afin de fournir un livrable adapté.
Une image `Docker` est un livrable qui n'est pas forcément intéressant
pour tous les publics. Certains préféreront avoir un plat bien préparé
qu'une recette ; certains apprécieront avoir une image `Docker` mais
d'autres ne seront pas en mesure de construire celle-ci ou ne sauront
pas la faire fonctionner. Une image `Docker` est plus souvent un
moyen pour faciliter la mise en service d'une production qu'une fin en soi.

Nous allons donc proposer
plusieurs types de livrables plus classiques par la suite. Ceux-ci
correspondront mieux aux attendus des publics utilisateurs de services
construits à partir de techniques de _data science_. `Docker` est néanmoins
un passage obligé car l'ensemble des types de livrables que nous allons
explorer reposent sur la standardisation permise par les conteneurs.

Cette approche nous permettra de quitter le domaine de l'artisanat pour
s'approcher d'une industrialisation de la mise à disposition
de notre projet. Ceci va notamment nous amener à mettre en oeuvre
l'approche pragmatique du `DevOps` qui consiste à intégrer dès la phase de
développement d'un projet les contraintes liées à sa mise à disposition
au public cible (cette approche est détaillée plus
amplement dans le chapitre sur la [mise en production](/chapters/deployment.qmd)).

L'automatisation et la mise à disposition automatisée de nos productions
sera faite progressivement, au cours des prochaines parties. Tous les
projets n'ont pas vocation à aller aussi loin dans ce domaine.
L'opportunité doit être comparée aux coûts humains et financiers
de leur mise en oeuvre et de leur cycle de vie.
Avant de faire une production en série de nos modèles,
nous allons déjà commencer
par automatiser quelques tests de conformité de notre code.
On va ici utiliser l'intégration continue pour deux objectifs distincts:

- la mise à disposition de l'image `Docker` ;
- la mise en place de tests automatisés de la qualité du code
sur le modèle de notre `linter` précédent.

Nous allons utiliser `Github Actions` pour cela. Il s'agit de serveurs
standardisés mis à disposition gratuitement par `Github` {{<fa brands github >}}.
`Gitlab` {{<fa brands gitlab >}}, l'autre principal acteur du domaine,
propose des services similaires. L'implémentation est légèrement différente
mais les principes sont identiques.


::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli13
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


## Étape 1: mise en place de tests automatisés

Avant d'essayer de mettre en oeuvre la création de notre image
`Docker` de manière automatisée, nous allons présenter la logique
de l'intégration continue en testant de manière automatisée
notre script `main.py`.

Pour cela, nous allons partir de la structure proposée dans l'[action officielle](https://github.com/actions/setup-python).
La documentation associée est [ici](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python).
Des éléments succincts de présentation de la logique déclarative des actions `Github`
sont disponibles dans le chapitre sur la [mise en production](/chapters/deployment.qmd). Néanmoins, la meilleure
école pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d'observer
les actions `Github` mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !


{{< include "./applications/_appli14.qmd" >}}


Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entraîner une série d'actions automatisées.

Si l'une des étapes échoue, ou si la note de notre projet est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi détecter,
en développant son projet, les moments où on dégrade la qualité du script
afin de la rétablir immédiatemment.



## Étape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise à disposition de notre image
sur `DockerHub` (le lieu de partage des images `Docker`). Cela facilitera sa réutilisation mais aussi des
valorisations ultérieures.

Là encore, nous allons utiliser une série d'actions pré-configurées.

Pour que `Github` puisse s'authentifier auprès de `DockerHub`, il va
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
sécurisé associé à votre dépôt `Github`.


{{< include "./applications/_appli15a.qmd" >}}


A ce stade, nous avons donné les moyens à `Github` de s'authentifier avec
notre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action
en s'inspirant de la [documentation officielle](https://github.com/docker/build-push-action/#usage).
On ne va modifier que trois éléments dans ce fichier. Effectuer les
actions suivantes:


{{< include "./applications/_appli15b.qmd" >}}



# Partie 5: expérimenter en local des valorisations puis automatiser leur production


Nous avons automatisé les étapes intermédiaires de notre projet.
Néanmoins nous n'avons pas encore réfléchi à la valorisation
à mettre en oeuvre pour notre projet. On va supposer que notre
projet s'adresse à des _data scientists_ mais aussi à une audience
moins technique. Pour ces premiers, nous pourrions nous contenter
de valorisations techniques, comme des API,
mais pour ces derniers il est
conseillé de privilégier des formats plus _user friendly_.

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli15
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


Afin de faire le parallèle avec les parcours possibles pour l'évaluation,
nous allons proposer trois valorisations[^valorisation]:

- Une [API](https://titanic.kub.sspcloud.fr/docs) facilitant la réutilisation du modèle en "production" ;
- Un [site web statique](https://ensae-reproductibilite.github.io/application/) exploitant cette API pour exposer les prédictions
à une audience moins technique.


[^valorisation]: Vous n'êtes pas obligés pour l'évaluation de mettre en oeuvre
les jalons de plusieurs parcours. Néanmoins, vous découvrirez que
chaque nouveau pas en avant est moins coûteux que le
précédent si vous avez mis en oeuvre les réflexes des bonnes
pratiques.



::: {.callout-warning collapse="true"}
## Site statique vs application réactive

La solution que nous allons proposer
pour les sites statiques, `Quarto` associé
à `Github Pages`, peut être utilisée dans le cadre des parcours
_"rapport reproductible"_ ou _"dashboard / application interactive"_.

Pour ce dernier
parcours, d'autres approches techniques sont néanmoins possibles,
comme `Streamlit`. Celles-ci sont plus exigeantes sur le plan technique
puisqu'elles nécessitent de mettre en production sur des serveurs
conteuneurisés (comme la mise en production de l'API)
là où le site statique ne nécessite qu'un serveur web, mis à disposition
gratuitement par `Github`.


La distinction principale entre ces deux approches est qu'elles
s'appuient sur des serveurs différents. Un site statique repose
sur un serveur web là où `Streamlit` s'appuie sur
serveur classique en _backend_. La différence principale
entre ces deux types de serveurs
réside principalement dans leur fonction et leur utilisation:

- Un __serveur web__ est spécifiquement conçu pour stocker, traiter et livrer des pages web aux clients. Cela inclut des fichiers HTML, CSS, JavaScript, images, etc. Les serveurs web écoutent les requêtes HTTP/HTTPS provenant des navigateurs des utilisateurs et y répondent en envoyant les données demandées.
- Un **serveur _backend_** classique est conçu pour effectuer des opérations en réponse à un _front_, en l'occurrence une page web.
Dans le contexte d'une application `Streamlit`, il s'agit d'un serveur avec l'environnement `Python` _ad hoc_ pour
exécuter le code nécessaire à répondre à toute action d'un utilisateur de l'appliacation.

:::


## Étape 1: développer une API en local

Le premier livrable devenu classique dans un projet
impliquant du _machine learning_ est la mise à
disposition d'un modèle par le biais d'une
API (voir chapitre sur la [mise en production](/chapters/deployment.qmd)).
Le _framework_ [`FastAPI`](https://fastapi.tiangolo.com/) va permettre
de rapidement transformer notre application `Python` en une API fonctionnelle.

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli15
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


{{< include "./applications/_appli16.qmd" >}}


## Étape 2: déployer l'API de manière manuelle

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli16
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

A ce stade, nous avons déployé l'API seulement localement, dans le cadre d'un terminal qui tourne en arrière-plan.
C'est une mise en production manuelle, pas franchement pérenne.
Ce mode de déploiement est très pratique pour la phase de développement, afin de s'assurer que l'API fonctionne comme attendu.
Pour pérenniser la mise en production, on va éliminer l'aspect artisanal de celle-ci.

Il est temps de passer à l'étape de déploiement, qui permettra à notre API d'être accessible via une URL sur le web
et d'avoir un serveur, en arrière plan, qui effectuera les opérations pour répondre à une
requête. Pour se faire, on va utiliser les possibilités offertes par `Kubernetes`, sur lequel est basé le [SSP Cloud](https://datalab.sspcloud.fr).


{{< include "./applications/_appli17.qmd" >}}


Nous avons préparé la mise à disposition de notre API mais à l'heure
actuelle elle n'est pas disponible de manière aisée car il est nécessaire
de lancer manuellement une image `Docker` pour pouvoir y accéder.
Ce type de travail est la spécialité de `Kubernetes` que nous allons
utiliser pour gérer la mise à disposition de notre API.

{{< include "./applications/_appli18.qmd" >}}

::: {.callout-note title="Gérer le CORS" collapse="true"}
Notre API est accessible sans problème depuis `Python` ou notre navigateur.

En revanche, si on désire utiliser `JavaScript` pour créer une application
interactive il est indispensable de mettre
les lignes un peu obscure sur le CORS dans le fichier `ingress.yaml`.

Comme c'est un point technique qui ne concerne pas les compétences
liées à ce cours, nous donnons directement les mises à jour nécessaires
du projet.

Ceci consiste principalement à ajouter la ligne suivante au fichier `ingress.yaml` :

```
nginx.ingress.kubernetes.io/enable-cors: "true"
```

:::


On peut remarquer quelques voies d'amélioration de notre approche qui
seront ultérieurement traitées:

- L'entraînement du modèle
est ré-effectué à chaque lancement d'un nouveau conteneur.
On relance donc autant de fois un entraînement qu'on déploie
de conteneurs pour répondre à nos utilisateurs. Ce sera
l'objet de la partie MLOps de fiabiliser et optimiser
cette partie du _pipeline_.
- il est nécessaire de (re)lancer manuellement  `kubectl apply -f deployment/`
à chaque changement de notre code. Autrement dit, lors de cette application,
on a amélioré
la fiabilité du lancement de notre API mais un lancement manuel est encore indispensable.
Comme dans le reste de ce cours, on va essayer d'éviter un geste manuel pouvant
être source d'erreur en privilégiant l'automatisation et l'archivage dans des
scripts. C'est l'objet de la prochaine étape.


## Etape 3: automatiser le déploiement (déploiement en continu)

::: {.callout-important}
## Clarification sur la branche de travail et les _tags_

A partir de maintenant, il est nécessaire de clarifier la
branche principale sur laquelle nous travaillons. De manière
traditionnelle, on utilise la branche `main`. Si vous avez changé de branche,
vous pouvez continuer 1/ continuer mais en tenir compte dans les exemples ultérieurs ou 2/ fusionner celle-ci à `main`.

Si vous avez utilisé un `tag` pour sauter une ou plusieurs étapes, il va
être nécessaire de se placer sur une branche car vous êtes en _head detached_.
Pour cela, après avoir _committé_ les fichiers que vous désirez garder

```{python}
#| eval: false
#| file: "terminal"
#| filename: "terminal"
$ git branch -D dev #<1>
$ git push origin -d dev #<2>
$ git checkout -b dev #<3>
$ git push --set-upstream origin dev #<4>
```
1. Supprime la branche `dev` *locale* (si elle existe).
2. Supprime la branche `dev` *remote* (si elle existe).
3. Crée une nouvelle branche `dev` *locale* et on se place sur cette branche.
4. Pousse la branche `dev` et active la synchronisation entre la branche *locale* et la branche *remote*.
:::

Qu'est-ce qui peut déclencher une évolution nécessitant de mettre à jour l'ensemble de notre processus de production ?

Regardons à nouveau notre _pipeline_:

![](/drawio/end_point.png)

Les _inputs_ de notre _pipeline_ sont donc:

- La __configuration__. Ici, on peut considérer que notre `.env` de configuration ou les secrets renseignés à `Github` relèvent de cette catégorie  ;
- Les __données__. Nos données sont statiques et n'ont pas vocation à évoluer. Si c'était le cas, il faudrait en tenir compte dans notre automatisation[^versionning-data]. ;
- Le __code__. C'est l'élément principal qui évolue chez nous. Idéalement, on veut automatiser le processus au maximum en faisant en sorte qu'à chaque mise à jour de notre code (un _push_ sur `Github`), les étapes ultérieures (production de l'image `Docker`, etc.) se lancent. Néanmoins, on veut aussi éviter qu'une erreur puisse donner lieu à une mise en production non-fonctionnelle, on va donc maintenir une action manuelle minimale comme garde-fou.

::: {.callout-note}
## Et le _versionning_ des données ?

Ici, nous nous plaçons dans le cas simple où les données brutes reçues sont figées. Ce qui peut changer est la manière dont on constitue nos échantillons train/test. Il sera donc utile de logguer les données en question par le biais de `MLFlow`. Mais il n'est pas nécessaire de versionner les données brutes.

Si celles-ci évoluaient, il pourrait être utile de versionner les données, à la manière dont on le fait pour le code. `Git` n'est pas l'outil approprié pour cela. Parmi les outils populaires de versionning de données, bien intégrés avec S3, il y a sur le SSPCloud [`lakefs`](https://lakefs.io/).
:::

Pour automatiser au maximum la mise en production, on va utiliser un nouvel outil : `ArgoCD`. Ainsi, au lieu de devoir appliquer manuellement la commande `kubectl apply` à chaque modification des fichiers de déploiement (présents dans le dossier `kubernetes/`), c'est l'**opérateur** `ArgoCD`, déployé sur le cluster, qui va détecter les changements de configuration du déploiement et les appliquer automatiquement.

C'est l'approche dite **GitOps** : le dépôt `Git` du déploiement fait office de **source de vérité unique** de l'état voulu de l'application, tout changement sur ce dernier doit donc se répercuter immédiatement sur le déploiement effectif.

{{< include "./applications/_appli19a.qmd" >}}

A présent, nous avons tous les outils à notre disposition pour construire un vrai **pipeline de CI/CD, automatisé de bout en bout**. Il va nous suffire pour cela de mettre à bout les composants :

- dans la partie 4 de l'application, nous avons construit un **pipeline de CI** : on a donc seulement à faire un commit sur le dépôt de l'application pour lancer l'étape de **build** et de mise à disposition de la nouvelle image sur le `DockerHub` ;

- dans l'application précédente, nous avons construit un **pipeline de CD** : `ArgoCD` suit en permanence l'état du dépôt `GitOps`, tout commit sur ce dernier lancera donc automatiquement un redéploiement de l'application.

Il y a donc un élément qui fait la liaison entre ces deux pipelines et qui nous sert de garde-fou en cas d'erreur : la **version de l'application**.

{{< include "./applications/_appli19b.qmd" >}}




## Etape 4: construire un site web

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli19
git checkout -b dev
git push origin dev
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

On va proposer un nouveau livrable pour parler à un public plus large.
Pour faire ce site web,
on va utiliser `Quarto` et déployer sur `Github Pages`.

{{< include "./applications/_appli20.qmd" >}}


# Partie 6: adopter une approche MLOps pour améliorer notre modèle

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli20
git checkout -b dev
git push origin dev
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

Maintenant que nous avons tout préparé pour mettre à disposition rapidement un modèle,
nous pouvons revenir en arrière pour améliorer ce modèle. Pour cela, nous allons mettre en oeuvre une validation croisée.

Le problème que nous allons rencontrer va être que nous voudrions facilement tracer les évolutions de notre modèle, la qualité prédictive de celui-ci dans différentes situations. Il s'agira d'à nouveau mettre en place du _logging_ mais, cette fois, de suivre la qualité du modèle et pas seulement s'il fonctionne. L'outil `MLFlow` va répondre à ce problème et va, au passage, fluidifier la mise à disposition du modèle de production, c'est-à-dire de celui qu'on désire mettre à disposition du public.

## Revenir sur le code d'entraînement du modèle pour faire de la validation croisée

Pour pouvoir faire ceci, il va falloir changer un tout petit peu notre code applicatif dans sa phase d'entraînement.

{{< include "./applications/_appli21.qmd" >}}


## Garder une trace des entraînements de notre modèle grâce au _register_ de `MLFlow`

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git stash
git checkout appli21
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

## Enregistrer nos premiers entraînements

{{< include "./applications/_appli22.qmd" >}}

Cette appplication illustre l'un des premiers apports de `MLFlow`: on garde
une trace de nos expérimentations: le modèle est archivé avec les paramètres et des métriques de performance. On peut donc retrouver de plusieurs manières un modèle qui nous avait tapé dans l'oeil.

Néanmoins, persistent un certain nombre de voies d'amélioration dans notre _pipeline_.

- On entraîne le modèle en local, de manière séquentielle, et en lançant nous-mêmes le script `train.py`.
- Pis encore, à l'heure actuelle, cette étape d'estimation n'est pas séparée de la mise à disposition du modèle par le biais de notre API. On archive des modèles mais on les utilise pas ultérieurement.


Les prochaines applications permettront d'améliorer ceci.

## Consommation d'un modèle archivé sur `MLFlow`

A l'heure actuelle, notre _pipeline_ est linéaire:

![](/chapters/applications/figures/pipeline_avant_appli23.png)

Ceci nous gêne pour faire évoluer notre modèle: on ne dissocie pas ce qui relève de l'entraînement du modèle de son utilisation. Un _pipeline_ plus cyclique permettra de mieux dissocier l'expérimentation de la production:

![](/chapters/applications/figures/pipeline_apres_appli23.png)


__TO BE CONTINUED__

{{< include "./applications/_appli23.qmd" >}}

::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git stash #<1>
git checkout appli23
```
1. Pour annuler les modifications depuis le dernier _commit_


![](/checkpoint.jpg){width=80% fig-align="center"}

:::

### Industrialiser les entraînements de nos modèles

Pour industrialiser nos entraînements, nous allons créer des processus
parallèles indépendants pour chaque combinaison de nos hyperparamètres.
Pour cela, l'outil pratique sur le SSPCloud est `Argo workflows`.
Chaque combinaison d'hyperparamètres sera un processus isolé à l'issue duquel sera
loggué le résultat dans `MLFlow`. Ces entraînements auront lieu en parallèle.


![](https://inseefrlab.github.io/formation-mlops/slides/img/pokemon_workflow.png)


1. Lancer un service Argo Workflows
2. Dans `mlflow/training.yaml`

```{python}
#| eval: false
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: titanic-training-workflow-
spec:
  entrypoint: main
  arguments:
    parameters:
      # The MLflow tracking server is responsible to log the hyper-parameter and model metrics.
      - name: mlflow-tracking-uri
        value: https://user-lgaliana-argo-workflows.user.lab.sspcloud.fr #<1>
      - name: mlflow-experiment-name
        value: titanicml #<2>
      - name: model-training-conf-list
        value: |
          [
            { "dim": 25, "lr": 0.1 },
            { "dim": 100, "lr": 0.2 },
            { "dim": 150, "lr": 0.3 }
          ]
  templates:
    # Entrypoint DAG template
    - name: main
      dag:
        tasks:
          # Task 0: Start pipeline
          - name: start-pipeline
            template: start-pipeline-wt
          # Task 1: Train model with given params
          - name: train-model-with-params
            dependencies: [ start-pipeline ]
            template: run-model-training-wt
            arguments:
              parameters:
                - name: dim
                  value: "{{item.dim}}"
                - name: lr
                  value: "{{item.lr}}"
            # Pass the inputs to the task using "withParam"
            withParam: "{{workflow.parameters.model-training-conf-list}}"

    # Now task container templates are defined
    # Worker template for task 0 : start-pipeline
    - name: start-pipeline-wt
      inputs:
      container:
        image: busybox
        command: [ sh, -c ]
        args: [ "echo Starting pipeline" ]

    # Worker template for task-1 : train model with params
    - name: run-model-training-wt
      inputs:
        parameters:
          - name: dim
          - name: lr
      container:
        image: inseefrlab/formation-mlops:main
        imagePullPolicy: Always
        command: [sh, -c]
        args: ["mlflow run .
                --env-manager=local
                -P remote_server_uri=$MLFLOW_TRACKING_URI
                -P experiment_name=$MLFLOW_EXPERIMENT_NAME
                -P dim={{inputs.parameters.dim}}
                -P lr={{inputs.parameters.lr}}"]
        env:
          - name: MLFLOW_TRACKING_URI
            value: "{{workflow.parameters.mlflow-tracking-uri}}"
          - name: MLFLOW_EXPERIMENT_NAME
            value: "{{workflow.parameters.mlflow-experiment-name}}"
```
1. Changer
2. `titanicml`

max_depth

max_features “sqrt”, “log2”

## Pour aller plus loin

Créer un service label studio pour évaluer la qualité du modèle
